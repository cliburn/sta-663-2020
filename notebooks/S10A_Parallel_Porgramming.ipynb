{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming\n",
    "\n",
    "This is an elementary treatment that covers simple use cases that often arise in statistics and data science, and is focused mostly on parallelism on a multi-core computer. More specialized frameworks for parallel programming such as those listed are not covered:\n",
    "\n",
    "- Message Passing Interface (MPI). Framework for parallelism based on message passing on a compute cluster. Often used when parallel processes need to frequently communicate with each other. More common in scientific than data analysis applications.\n",
    "- GPU computing (CUDA, OpenCL). Framework for exploiting the massively parallel but basic compute units available in modern GPUs. Ideal when you need a large number of simple arithmetic function calls - for example, in deep learning. The [CuPy](https://cupy.chainer.org) library looks like a promising GPU-accelerated alternative to `numpy` if you have a modern NVidia GPU.\n",
    "- Distributed computing (Spark, Dask). Framework for master-worker parallelism on a cluster of commodity machines. Master builds a graph of task dependencies and schedules to execute tasks in the appropriate order.\n",
    "\n",
    "This first lecture is for mainly illustrating concepts. The second will provide more examples of using parallelism in Python programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking machine capabilities on a Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sysctl hw.physicalcpu hw.logicalcpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Unix system, use this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lscpu --all --extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referneces**\n",
    "\n",
    "- [`threading` — Thread-based parallelism](https://docs.python.org/3.8/library/threading.html)\n",
    "- [`multiprocessing` - Process-based “threading” interface](https://docs.python.org/3.8/library/multiprocessing.html)\n",
    "- [`multiprocess` - a \"better\" `multiprocessing`](https://github.com/uqfoundation/multiprocess)\n",
    "- [concurrent.futures — Launching parallel tasks](https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures)\n",
    "- [Concurrency with Processes, Threads, and Coroutines](https://pymotw.com/3//concurrency.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common sense\n",
    "\n",
    "- Make it right before trying to make it fast\n",
    "- Weight the trade-off between programmer and program time\n",
    "- Don't blindly optimize - profile code before optimization\n",
    "- A new algorithm or data structure can reduce the complexity class; parallelism generally only offers linear speed ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "### Concurrent, parallel, distributed\n",
    "\n",
    "- Concurrent programs have tasks with overlapping lifetimes, and concurrent programming is based on units of execution that can be run in any order with the same final result. Concurrent programs can but need not be executed in parallel - for example, the time-slicing performed by the Python's Global Interpreter Lock (GIL), which only allows one thread to be physically executed at any point in time, is an example of concurrency.\n",
    "- Parallel programs run at the same time, for example, on multiple cores. If there is no need to control access to shared resources, these are known as *embarrassingly parallel* programs. All parallel programs are thus concurrent, but not vice versa.\n",
    "- Distributed programs run over multiple computers. The term is most often associated with \"Big Data\" problems, where data transfer is the computing bottleneck - hence the philosophy of \"bring the computation to the data\".\n",
    "\n",
    "### Synchronous and asynchronous calls\n",
    "\n",
    "When there is concurrent access to a shared resource, errors can arise. For example, consider the following code executed by two concurrent processes which share the variable `i` with an initial value of 0:\n",
    "\n",
    "```python\n",
    " i = i + 1\n",
    "```\n",
    "\n",
    "The following may happen: Process 1 reads (`i = 0`), Process 2 reads (`i = 0)`, Process 1 updates (`i = 1`), Process 2 overwrites the value put in by Process 1 (`i = 1`). Hence, after two updates, the value of `i` is 1 and not 2.\n",
    "\n",
    "To avoid this problem, the first process to access the resource may *lock* it for exclusive access:\n",
    "\n",
    "Process 1 reads and locks (`i = 0`), Process 2 tries to access `i` but it is locked, Process 1 updates and releases lock (`i = 1`), Process 2 now reads and locks (`i = 1`), and updates and unlocks (`i = 2`). Hence, after two updates, the value of `i` is 2 as it should be. Note that because the processes are *synchronous*, each process *blocks* access to the shared resource, forcing other process to wait - in Python, this waiting manifests as lack of access to the interpreter until the computation is complete.  In this context, *synchronous* and *blocking* have the same meaning.\n",
    "\n",
    "*Asynchronous* or *non-blocking* calls were designed to allow access to the interpreter at all times - an asynchronous function call returns control to the interpreter immediately, and returns a *future* object that can be used to check at a later point in time if the computation is completed and if so, to return the answer.\n",
    "\n",
    "### Concurrency is difficult\n",
    "\n",
    "Designing programs that share resources or have other dependencies to run concurrently in an effective way is challenging. One of the main reasons for the increasing popularity of functional programming is that concurrency is generally easier with a functional programming style - for example, the famous map-reduce framework for distributed computing uses the `map` and `reduce` abstractions of functional programming.\n",
    "\n",
    "Here we illustrate two famous problems that may arise in concurrent programs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race condition \n",
    "\n",
    "A *race condition* is one where the result depends on the order of accessing a shared resource, as in the simple example of updating `i` given above.\n",
    "\n",
    "In the example below, up to 4 processes may be trying to increment and assign a new value to val at the same time. Because this takes two steps (increment the RHS, assign to LHS), it can happen that two or more processes increment at the same time, but this is only assigned and counted once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the code works will become clear in the next lecture, but the basic idea is that we have 4 processes trying to increment a value, and because of race conditions, this gives the wrong final count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_with_value(i):\n",
    "    val.value += 1\n",
    "    \n",
    "val = Value('i', 0)\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_value, range(1000))\n",
    "\n",
    "val.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preventing race conditions by locking resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = Lock()\n",
    "\n",
    "def count_with_lock(i, lock=lock):\n",
    "    lock.acquire()\n",
    "    val.value += 1\n",
    "    lock.release()\n",
    "\n",
    "val = Value('i', 0)\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_lock, range(1000))\n",
    "\n",
    "val.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preventing race conditions by duplicating resources\n",
    "\n",
    "It is usually easier and faster to make copies of resources for each process so that no sharing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Array\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_with_array(i):\n",
    "    ix = mp.current_process().ident % 4\n",
    "    arr[ix] += 1\n",
    "    \n",
    "arr = Array('i', [0]*4)\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "    pool.map(count_with_array, range(1000))\n",
    "\n",
    "arr[:], sum(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deadlock\n",
    "\n",
    "Suppose each process needs both resources `i` and `j` to compute and return a value. Process 1 reads and locks `i` but before it can read `j`, Process 2 reads and locks `j`. Now both processes are waiting but cannot access the other shared resource, and hence the program hangs forever.\n",
    "\n",
    "There is a story about the dining philosophers to help you remember this - a group of philosophers are seated on a round table - on the left of each philosopher is a fork and on the right is a knife. Philosophers need a fork and a knife to eat. At the beginning, each philosopher grabs the utensil to the right - now none of the philosophers have both a knife and a fork, and they starve to death.\n",
    "\n",
    "To break deadlock, a possible strategy is to release locked resources upon failure and wait a random amount of time before trying to acquire it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl's and Gustafson's laws\n",
    "\n",
    "Suppose you had a million core machine. How much can you speed up your program by parallelizing it?\n",
    "\n",
    "Ahmdahl recognized that the degree of speed-up is driven by the ratio of serial to parallelziable code segments. Serial parts are those where the order is essential $a \\to b \\to c$. Suppose the serial section of code takes up 10% of the the total run-time. Then the maximum speed up 10-fold - the serial part takes up 10% of the time, and you throw one million minus one cores at the parallelizable parts driving their execution time towards zero, giving the 10-fold speed up.\n",
    "\n",
    "Gustafson pointed out that as the size of the problem increases, the time spent in  serial code segments typically goes down. For example, MCMC is inherently *all* serial, but for very large data sets, within each MCMC step, there is typically a need to calculate the likelihood of each data point, and this can be parallelized. We used this idea to achieve over two orders of magnitude speed up for fitting statistical mixture models using GPU computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opportunities for parallelism\n",
    "\n",
    "There are 3 common areas where parallelism can be useful in an algorithm:\n",
    "\n",
    "#### Data\n",
    "\n",
    "- Decomposition of arrays along rows, columns, blocks\n",
    "- Decomposition of trees into sub-trees\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "- Loops\n",
    "- Function calls\n",
    "\n",
    "#### Pipelines\n",
    "\n",
    "If there are a sequence of dependent stages, as soon as one stage completes and hands off to the next stage, it is ready to accept a job from the previous stage\n",
    "\n",
    "- Data processing pipeline\n",
    "\n",
    "$$\n",
    "a \\\\\n",
    "a \\to b \\\\\n",
    "a \\to b \\to c \\\\\n",
    "a \\to b \\to c \\\\\n",
    "a \\to b \\to c \\\\\n",
    "b \\to c \\\\\n",
    "c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common parallelization idioms\n",
    "\n",
    "- Loop parallelism automatically performs the tasks in a loop in parallel\n",
    "- Fork-join repeatedly splits execution into multiple branches (fork), then merging the branches (join), and is often used for recursive problems.\n",
    "- Single program multiple data (SPMD), where the same program runs on multiple cores on one or more computers, but process different inputs.\n",
    "- Master worker splits processes into a master that generates subproblems for multiple workers to complete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop parallelism with `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(n):\n",
    "    sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "[func(1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `joblib` library has a list comprehension idiom for loop parallelism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Parallel(n_jobs=4)(delayed(func)(1) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `delayed` function is just a technical device to allow function calls with the usual function call syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_args(n, *args, **kwargs):\n",
    "    sleep(n)\n",
    "    return args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Parallel(n_jobs=4)(delayed(func_args)(1, 2, 3, a=4, b=5) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lecture we will see the use of parallel ranges for loop parallelism with `nuumba` and `cython`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads and processes\n",
    "\n",
    "We can think of threads and processes as independent workers. The main difference is that threads run in the same memory space, while each process has its own memory space. This means that threads are \"light-weight\" compared to processes - they are faster to create and consume fewer resources.\n",
    "\n",
    "However, because threads run in the same memory space, concurrency issues such as race conditions and deadlock can occur. Because of this, the most commonly used Python interpreter (CPython) uses the [Global Interpreter Lock (GIL)](http://www.dabeaz.com/python/UnderstandingGIL.pdf) to ensure that only a *single* thread is operating in a Python program any one time, and switches between multiple threads to give the illusion of parallelism. The take-home message is that you can use threads for parallel tasks that are are slow because of latency (e.g. network request), but should use processes for parallel tasks that are compute-intensive (e.g. mathematical calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_with_latency(n):\n",
    "    sleep(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads work well for high latency processes - e.g. disk access, network requests, await user input etc - here simulated with `sleep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_with_latency, [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_with_latency, [1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not so well for compute-intensive processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_compute_intesnive(n):\n",
    "    s = 0\n",
    "    for i in range(1, n+1):\n",
    "        s += math.exp(math.log(math.sqrt(math.pow(i, 2.0))))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "func_compute_intesnive(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_compute_intesnive, [n,n,n,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(func_compute_intesnive, [n,n,n,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarrassingly parallel programs\n",
    "\n",
    "Many statistical problems can be easily decomposed into independent tasks or data sets. Here are several examples:\n",
    "\n",
    "- Monte Carlo integration\n",
    "- Multiple chains of MCMC\n",
    "- Bootstrap for confidence intervals\n",
    "- Power calculations by simulation\n",
    "- Permutation-resampling tests \n",
    "- Fitting same model on multiple data sets\n",
    "\n",
    "These \"low hanging fruits\" are great because they offer a path to easy parallelism with minimal complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
